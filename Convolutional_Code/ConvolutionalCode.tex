\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\lstset{
    language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{magenta},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b,
    tabsize=4,
    showstringspaces=false
}

\title{A Comprehensive Overview of Convolutional Encoders and the Viterbi Algorithm}
\author{Your Name}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}

In modern digital communication systems, transmitting information reliably over noisy channels is a fundamental challenge. Noise, interference, and signal degradation can lead to errors that corrupt the transmitted data. To mitigate these issues, \emph{error-correcting codes} are employed. These codes add structured redundancy to the data, enabling the receiver to detect and, in many cases, correct errors introduced during transmission.

Among a variety of coding techniques, \emph{convolutional codes} are particularly noteworthy due to their continuous processing of data and their adaptability to various channel conditions. Instead of encoding data in fixed-length blocks, convolutional encoders treat the input stream as a continuous flow, using internal memory elements to produce a code sequence that depends not only on the current input bits but also on a portion of the past input.

To decode convolutionally encoded data, one of the most prominent algorithms is the \emph{Viterbi algorithm}. This algorithm provides a maximum-likelihood decoding approach, meaning it selects the most likely transmitted sequence given the noisy received signal. By effectively searching through a trellis of possible states and transitions, the Viterbi algorithm can reliably recover the original information bits with a high degree of accuracy.

This document will provide a deeply detailed exploration of convolutional encoders and the Viterbi algorithm, covering fundamental concepts, theoretical foundations, and practical insights, including:
\begin{itemize}
    \item A thorough introduction to convolutional encoders, including their internal structure and key parameters.
    \item The concept of \emph{free distance} and its importance in defining the error-correcting capability of a convolutional code.
    \item A step-by-step explanation of the Viterbi algorithm, including its trellis representation, path metrics, and traceback procedure.
    \item An illustrative C++ code example demonstrating how to implement a convolutional encoder, simulate a noisy channel, and decode using the Viterbi algorithm.
\end{itemize}

\section{Convolutional Encoders}

\subsection{Definition and Intuition}

A \textbf{convolutional encoder} is a type of forward error-correcting encoder that processes a continuous stream of input bits. Unlike block codes, where the input is partitioned into distinct blocks before encoding, convolutional codes incorporate memory. In other words, the output at any given time depends not only on the current input bit but also on some number of previous bits. This memory aspect creates a \emph{convolution}—a sliding operation—of the input sequence with a set of generator polynomials.

Visually, a convolutional encoder can be represented by shift registers and XOR gates:
\begin{itemize}
    \item The shift registers hold a certain number of past input bits, representing the encoder's memory.
    \item Each output bit is generated by XOR-ing certain taps (positions) in the register as specified by the generator polynomials.
\end{itemize}
Because the output at each time step depends on a combination of current and past inputs, small input changes can affect multiple subsequent output bits, distributing the information and redundancy throughout the transmitted sequence.

\subsection{Key Parameters}

\paragraph{Constraint Length ($k$):}  
The \emph{constraint length} determines the number of past input bits (plus the current bit) that the encoder memory considers. If the encoder's memory size is $k-1$ bits, then $k$ is the total number of bits (current input plus memory) that determine the output at any instant.

For instance, if $k=3$, the encoder remembers two previous input bits and the current one, making the total span of influence three bits at a time.

\paragraph{Code Rate ($R$):}  
The \emph{rate} of a convolutional code is defined as:
\[
R = \frac{k_{\text{in}}}{n_{\text{out}}},
\]
where $k_{\text{in}}$ is the number of input bits processed at each time step (often 1) and $n_{\text{out}}$ is the number of output bits produced at each time step. For many practical convolutional encoders, $k_{\text{in}}=1$, so the rate simplifies to $R = 1/n_{\text{out}}$. A lower rate (e.g., $1/3$) means more redundancy is added, potentially improving error correction at the cost of higher bandwidth usage.

\paragraph{Generator Polynomials:}  
The behavior of a convolutional encoder is fully determined by its \textbf{generator polynomials}. Each output bit is associated with a polynomial that dictates which taps from the memory (including the current input) are XOR-ed together. For example, if the constraint length is $k=3$, and you have one output polynomial $G(D) = 1 + D + D^2$, it means the output bit is formed by XOR-ing the current input bit and the two previous bits in the register.

Multiple polynomials produce multiple outputs at each time step, creating a coded output sequence that combines information from the current and previous input bits in a structured manner.

\subsection{Free Distance and Error-Correcting Capability}

The traditional concept of \emph{minimum distance} used in block codes is adapted in convolutional codes as the \emph{free distance}, $d_{\text{free}}$. The free distance is the minimum Hamming distance between any two distinct infinite-length encoded sequences. Since convolutional codes operate on streams rather than blocks, this infinite-length consideration is essential.

The free distance sets a fundamental limit on the code's ability to distinguish transmitted sequences from one another after passing through a noisy channel. The larger the free distance, the more resilient the code is to errors. A high free distance implies that valid encoded sequences are more widely separated in Hamming space, making it easier for the decoder to identify the correct sequence despite noise-induced errors.

\section{The Viterbi Algorithm}

\subsection{Overview and Motivation}

The \textbf{Viterbi algorithm}, introduced by Andrew Viterbi in 1967, revolutionized the decoding of convolutional codes. It provides a maximum-likelihood decoding approach, ensuring that the decoded sequence is the most probable one given the received noisy sequence. The Viterbi algorithm is optimal, meaning it finds the path through the state trellis that yields the minimal cumulative distance to the received sequence.

\subsection{The Trellis Diagram}

To decode convolutional codes, we represent the code as a \emph{trellis}, a time-indexed graph where:
\begin{itemize}
    \item Each column of the trellis corresponds to a time step.
    \item Each node in the column represents a possible state of the encoder (the contents of its memory).
    \item Each arrow (branch) connecting states from one time step to the next represents a possible input bit that transitions the encoder from one state to another. Each branch also has an associated output (encoded bits).
\end{itemize}

Since the encoder has memory of length $k-1$, there are $2^{k-1}$ possible states at any given time. Each new input bit (0 or 1) typically splits from each current state into one or two possible next states.

\subsection{How the Viterbi Algorithm Works in Detail}

1. \textbf{Initialization:}  
   At the start ($t=0$), the encoder is assumed to be in the all-zero state. The Viterbi decoder initializes the cumulative path metric (often the sum of Hamming distances) for the zero state to 0 and sets the metrics for all other states to infinity. This enforces that initially, the only considered valid path is the one that starts from the all-zero state.

2. \textbf{Branch Metrics and Path Metrics:}  
   For each received encoded symbol (which may be corrupted by noise), the decoder computes the \emph{branch metric}. The branch metric is often the Hamming distance between the expected encoded output for a given state transition and the actual received bits at that time step.

   After computing branch metrics, the algorithm updates the \emph{path metrics} (cumulative metrics) for each state by adding the branch metric to the metric of the previous path that leads to this state. Since there might be multiple incoming paths to a state, the decoder selects the path with the smallest cumulative metric as the \emph{survivor path}. This ensures that after each time step, only the most likely path leading into each state is retained.

3. \textbf{Survivor Paths and Traceback:}  
   As the decoding progresses over many time steps, the Viterbi algorithm maintains a record (often stored in traceback memory) of which path survives at each state. After processing all received data, the decoder will pick the state with the lowest final cumulative metric. To recover the transmitted bits, it performs a traceback through the survivor paths, moving backward in time until it reaches the initial state. This traceback reveals the most likely sequence of input bits.

   The Viterbi algorithm's computational complexity is linear in the length of the input sequence, but it grows exponentially with constraint length because of the number of states. In practice, $k$ is chosen to be moderately small (often less than or equal to 10) to keep decoding manageable.

\section{Example C++ Implementation}

Below is an example C++ code outline that demonstrates a simplified end-to-end system:
\begin{enumerate}
    \item \textbf{Input}: Reads a user-provided message as a string.
    \item \textbf{Binary Conversion}: Converts this string into a binary vector of bits for processing.
    \item \textbf{Encoding}: Applies a convolutional encoder (with a specified constraint length $k$ and a set of generator polynomials) to produce a coded bitstream.
    \item \textbf{Noise Simulation}: Introduces random bit errors into the encoded bitstream with probability $p$, simulating a noisy transmission channel.
    \item \textbf{Viterbi Decoding}: Uses the Viterbi algorithm to decode the noisy received bitstream and attempts to recover the original input message.
    \item \textbf{Performance Evaluation}: Computes performance metrics such as Bit Error Rate (BER) and success rate for different constraint lengths. These metrics help assess the effectiveness of the code under given channel conditions.
\end{enumerate}

\textbf{Note}: The following code snippet is a shortened, illustrative example. In a real-world scenario, you should ensure:
\begin{itemize}
    \item The C++ file (e.g., \texttt{ConvCode.cpp}) is located in the same directory as this \LaTeX{} file or that you provide the correct path.
    \item Proper compiler installation (e.g., \texttt{g++}) and environment setup.
\end{itemize}

\lstinputlisting[caption={C++ Code for Convolutional Encoding and Viterbi Decoding}, label=code:main]{ConvCode.cpp}

\section{How to Compile and Run the Code}

Assuming a standard Unix-like environment with a C++ compiler:
\begin{verbatim}
g++ -o conv_decoder ConvCode.cpp -O2 -std=c++11
./conv_decoder
\end{verbatim}

When running the executable:
\begin{enumerate}
    \item You will be prompted to input a message (a string of characters).
    \item The program encodes the message using the convolutional encoder.
    \item It then simulates channel noise by flipping bits with probability $p$.
    \item The Viterbi decoder attempts to recover the original message from the noisy encoded data.
    \item Finally, the program prints out the Bit Error Rate (BER) and success rate for various constraint lengths $k$, showing how code complexity and memory depth affect error-correction performance.
\end{enumerate}

\section{Conclusion}

Convolutional codes offer a powerful mechanism to combat channel noise by spreading input data across multiple output bits, thereby creating redundancy that can be exploited at the receiver. The Viterbi algorithm provides an optimal solution for decoding these codes, ensuring minimal decoding errors by selecting the most probable transmitted sequence based on a trellis representation of the encoding process.

In practical communication systems, convolutional codes and Viterbi decoding are widely used due to their balance of performance, complexity, and robustness. Applications range from deep-space communications (e.g., NASA missions) and satellite links to cellular networks and wireless LANs. Although newer code families like Turbo codes and LDPC codes have gained popularity, the foundational understanding of convolutional codes and the Viterbi algorithm remains an essential part of modern digital communications theory and practice.

\section*{References}

\begin{enumerate}
    \item J. G. Proakis, \emph{Digital Communications}, 4th ed., McGraw-Hill, 2001.
    \item S. Lin and D. J. Costello, Jr., \emph{Error Control Coding: Fundamentals and Applications}, 2nd ed., Prentice Hall, 2004.
    \item A. J. Viterbi, ``Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,'' \emph{IEEE Transactions on Information Theory}, vol. 13, no. 2, pp. 260--269, 1967.
    \item J. Hagenauer, ``The Viterbi Algorithm,'' \emph{Proceedings of the IEEE}, vol. 69, no. 11, pp. 1397--1449, 1981.
    \item G. David Forney Jr., ``The Viterbi Algorithm,'' \emph{Proceedings of the IEEE}, vol. 61, no. 3, pp. 268--278, 1973.
\end{enumerate}

\end{document}
