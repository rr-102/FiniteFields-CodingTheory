\documentclass[12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\lstset{
    language=C++,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{magenta},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b
}

\title{An Introduction to Convolutional Encoders and the Viterbi Algorithm}
\author{Your Name}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\section{Introduction}

Error-correcting codes are essential for reliable communication over noisy channels. Among these codes, \emph{convolutional codes} are widely used due to their capacity to handle continuous streams of data with a sliding-window approach. Unlike block codes, where the data is segmented into fixed-size blocks, convolutional encoders process input bits and produce a continuous encoded output sequence. This encoding process adds controlled redundancy, improving the receiver's ability to correct errors introduced by the channel.

The \emph{Viterbi algorithm} is a maximum likelihood decoding algorithm designed to decode convolutionally encoded sequences. It exploits the structure of the convolutional code to perform efficient, dynamic programming-based decoding. This document provides an overview of convolutional encoders, convolutional codes, and explains in detail how the included C++ code implements a convolutional encoder, introduces noise, and applies Viterbi decoding to retrieve the original message.

\section{What is a Convolutional Encoder?}

A convolutional encoder is defined by three main parameters:
\begin{enumerate}
    \item The \textbf{constraint length}, $k$: This represents the encoder's memory. Typically, $k$ represents how many previous input bits the encoder considers when producing its output. The encoder can be visualized as a shift register of length $k-1$ plus the current input bit, making $k$ bits in total.
    \item The \textbf{generator polynomials}: Each output bit of a convolutional encoder is generated by XOR-ing selected input and memory bits according to a predefined polynomial. For $n$ output bits per input bit, you have $n$ generator polynomials.
    \item The \textbf{rate}, $R = k/n$: Here $k$ is the number of input bits processed at a time (often $k=1$), and $n$ is the number of output bits generated for each input bit.
\end{enumerate}

In simpler terms, a convolutional encoder takes one input bit at a time, appends it to a memory of previous bits, and then generates several output bits by combining the current and past bits according to the generator polynomials. As a result, each input bit influences not just the immediate output but a sequence of subsequent outputs until it shifts out of the encoder's memory.

\subsection{Free Distance and Error-Correcting Capability}

Convolutional codes do not have a fixed minimum Hamming distance like block codes. Instead, they are characterized by a \textbf{free distance}, $d_{\text{free}}$, which is the minimum Hamming distance between any two infinitely long code sequences produced by distinct input sequences. The free distance governs the error-correcting capability: a larger free distance generally implies better error-correction performance. Intuitively, $d_{\text{free}}$ determines how far apart valid code sequences are in terms of bit differences, making it easier for the Viterbi algorithm to distinguish the correct path during decoding.

\section{The Viterbi Algorithm for Decoding Convolutional Codes}

The Viterbi algorithm is a dynamic programming algorithm that finds the most likely sequence of states in a Markov process, given an observed sequence of outputs. In the context of convolutional codes:
\begin{itemize}
    \item Each possible state of the encoder's memory (i.e., the contents of the shift register) forms a node in a \emph{trellis diagram}.
    \item Transitions between states correspond to input bits, and each transition produces an output codeword.
    \item The received sequence (potentially corrupted by noise) is compared against the possible output sequences at each time step.
    \item The Viterbi algorithm computes cumulative metrics (e.g., Hamming distances) for all possible paths through the trellis and selects the path with the minimum cumulative distance, effectively finding the most likely transmitted bit sequence.
\end{itemize}

Because the Viterbi algorithm uses dynamic programming, it avoids exponential complexity by merging paths that lead to the same state and only keeping track of the path with the lowest cumulative distance for each state at each time step.

\section{Overview of the Provided C++ Code}

The provided code:
\begin{enumerate}
    \item \textbf{Reads an input message} as a string.
    \item \textbf{Converts the message into a binary vector} (a sequence of bits).
    \item \textbf{Encodes the binary sequence} using a convolutional encoder defined by a set of generator polynomials and a constraint length $k$.
    \item \textbf{Adds noise} to the encoded sequence with a specified probability of error $p$.
    \item \textbf{Decodes the noisy sequence} using the Viterbi algorithm to estimate the original transmitted message.
    \item \textbf{Computes performance metrics} such as Bit Error Rate (BER) and success rate for various values of $k$.
\end{enumerate}

\section{Detailed Explanation of the Code}

Below is the code with detailed comments and explanations:

\lstinputlisting[caption={C++ Code for Convolutional Encoding and Viterbi Decoding}, label=code:main]{ConvCode.cpp}


\subsection{Key Functions and Data Structures}

\begin{description}
    \item[\texttt{map<int, vector<unsigned int>> generatorPolynomialsMap}] 
    Stores generator polynomials for different constraint lengths $k$. Each entry maps a specific $k$ to a set of identical generator polynomials for simplicity.

    \item[\texttt{struct vNode}]
    Represents a node in the trellis. It contains:
    \begin{itemize}
        \item \texttt{cumHammingDistance}: The cumulative metric (distance) from the start of the sequence to this state at a given time step.
        \item \texttt{inputArrivalBit}: The input bit that led to this node, useful for backtracking.
        \item \texttt{state}: The integer representation of the encoder state.
    \end{itemize}

    \item[\texttt{encode()}]
    Encodes a given binary sequence using the convolutional encoder. For each input bit, it:
    \begin{enumerate}
        \item Sets up a sliding window (shift register).
        \item Uses \texttt{generateOutput()} with the current register state and generator polynomials to produce the encoded output bits.
    \end{enumerate}

    \item[\texttt{viterbiDecode()}]
    Implements the Viterbi algorithm:
    \begin{enumerate}
        \item Initializes a trellis (2D structure) with states for each time step.
        \item At each time step, it updates the cumulative Hamming distances for all states based on the observed noisy encoded bits and the expected encoded bits from potential previous states.
        \item After processing all time steps, it uses backtracking (\texttt{getOriginalCode()}) to find the most likely transmitted input sequence.
    \end{enumerate}

    \item[\texttt{calculateHammingDistance()}]
    Computes the Hamming distance between two binary code sequences.

    \item[\texttt{generateOutput()}]
    Given a shift register (state) and a set of generator polynomials, it produces the encoded bits for the current input.

    \item[\texttt{addNoise()}]
    Simulates a noisy channel by flipping bits with probability $p$.

    \item[\texttt{generateStates()}]
    Generates all possible states of the encoder's memory (i.e., all combinations of $(k-1)$ bits).

    \item[\texttt{calculatePotentialInput()}]
    Given a current state and the next input bit, determines the full input vector (current memory + next bit).

    \item[\texttt{stringToVecBool()} and \texttt{vecBoolToString()}]
    Convert between strings and binary sequences represented as \texttt{std::vector<bool>}.

    \item[\texttt{exportData()}]
    Exports the summarized results for different $k$ values to a CSV file for further analysis.
\end{description}

\subsection{Performance Analysis}

The code executes multiple iterations for different $k$ values, each time:
\begin{enumerate}
    \item Encoding a message.
    \item Transmitting it through a simulated noisy channel with a given noise probability $p$.
    \item Decoding with the Viterbi algorithm.
\end{enumerate}

It calculates the Bit Error Rate (BER) and the success rate (percentage of times the entire message is recovered exactly). These results are then displayed and optionally exported for later analysis.

\section{How to Compile and Run the Code}

To compile:
\begin{verbatim}
g++ -o viterbi main.cpp -O2 -std=c++11
\end{verbatim}

To run:
\begin{verbatim}
./viterbi
\end{verbatim}

You will be prompted to enter a message. The program will then process the given message for different values of $k$ and print out comprehensive statistics, including BER and success rates.

\section{Conclusion}

Convolutional codes introduce redundancy by continuously encoding input bits and maintaining a memory of previous inputs. The Viterbi algorithm then decodes the resulting sequence by finding the most likely path through the state trellis. The provided code demonstrates the end-to-end process of encoding, adding noise, and decoding using the Viterbi algorithm. By experimenting with different parameters (such as $k$, the probability of error, and various generator polynomials), one can gain insights into the error-correction performance of convolutional codes and the efficiency of the Viterbi algorithm.

\section*{References}

\begin{enumerate}
    \item J. G. Proakis, ``Digital Communications,'' 4th ed., McGraw-Hill, 2001.
    \item S. Lin and D. J. Costello, Jr., ``Error Control Coding: Fundamentals and Applications,'' 2nd ed., Prentice Hall, 2004.
    \item A. J. Viterbi, ``Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,'' IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260--269, 1967.
\end{enumerate}

\end{document}
